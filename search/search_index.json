{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"buy.html","title":"Buy","text":"<p>Get your Jetson developer kit to try generative AIs in your own local environment.</p> <p>Note</p> <p>The availablility and the price of Jetson developer kits vary depending on your region.Please click \"Jetson Store\" button to find out the availability on the page next.</p> Product Action Jetson AGX Orin 64GB Developer Kit<ul><li>AI Perf: 275 TOPS</li><li>GPU: NVIDIA Ampere architecture with 2048 NVIDIA CUDA cores and 64 tensor cores</li><li>CPU: 12-core Arm Cortex-A78AE v8.2 64-bit CPU 3MB L2 + 6MB L3</li><li>Memory: 64GB 256-bit LPDDR5 | 204.8 GB/s</li><li>Storage: 64GB eMMC 5.1</li></ul>  Jetson Store  Jetson AGX Orin Developer Kit<ul><li>AI Perf: 275 TOPS</li><li>GPU: NVIDIA Ampere architecture with 2048 NVIDIA CUDA cores and 64 tensor cores</li><li>CPU: 12-core Arm Cortex-A78AE v8.2 64-bit CPU 3MB L2 + 6MB L3</li><li>Memory: 32GB 256-bit LPDDR5 | 204.8 GB/s</li><li>Storage: 64GB eMMC 5.1</li></ul>  Jetson Store  Jetson Orin Nano Developer Kit<ul><li>AI Perf: 40 TOPS</li><li>GPU: 1024-core NVIDIA Ampere architecture GPU with 32 Tensor Cores</li><li>CPU: 6-core Arm\u00ae Cortex\u00ae-A78AE v8.2 64-bit CPU 1.5MB L2 + 4MB L3</li><li>Memory: 8GB 128-bit LPDDR5  68 GB/s</li><li>Storage: SD Card Slot &amp; external NVMe via M.2 Key M</li></ul>  Jetson Store"},{"location":"getting-started.html","title":"Getting started","text":""},{"location":"tips_ram-optimization.html","title":"RAM Optimization","text":"<p>Running a LLM requires a huge RAM space.</p> <p>Especially if you are on Jetson Orin Nano that only has 8GB of RAM, it is crucial to leave as much RAM space available for models. </p> <p>Here we share a couple of ways to optimize the system RAM usage. </p>"},{"location":"tips_ram-optimization.html#disabling-the-desktop-gui","title":"Disabling the Desktop GUI","text":"<p>If you use your Jetson remotely through SSH, you can disable the Ubuntu desktop GUI. This will free up extra memory that the window manager and desktop uses (around ~800MB for Unity/GNOME).</p> <p>You can disable the desktop temporarily, run commands in the console, and then re-start the desktop when desired:</p> <pre><code>$ sudo init 3     # stop the desktop\n# log your user back into the console (Ctrl+Alt+F1, F2, ect)\n$ sudo init 5     # restart the desktop\n</code></pre> <p>If you wish to make this persistent across reboots, you can use the following commands to change the boot-up behavior:</p> <ul> <li> <p>To disable desktop on boot</p> <pre><code>$ sudo systemctl set-default multi-user.target     \n</code></pre> </li> <li> <p>To enable desktop on boot</p> <pre><code>$ sudo systemctl set-default graphical.target      \n</code></pre> </li> </ul>"},{"location":"tips_ram-optimization.html#disabling-misc-services","title":"Disabling misc services","text":"<pre><code>sudo systemctl disable nvargus-daemon.service\n</code></pre>"},{"location":"tips_ram-optimization.html#mounting-swap","title":"Mounting Swap","text":"<p>If you're building containers or working with large models, it's advisable to mount SWAP (typically correlated with the amount of memory in the board). Run these commands to disable ZRAM and create a swap file:</p> <p>If you have NVMe SSD storage available, it's preferred to allocate the swap file on the NVMe SSD.</p> <pre><code>sudo systemctl disable nvzramconfig\nsudo fallocate -l 16G /ssd/16GB.swap\nsudo mkswap /ssd/16GB.swap\nsudo swapon /ssd/16GB.swap\n</code></pre> <p>Then add the following line to the end of /etc/fstab to make the change persistent:</p> <pre><code>/ssd/16GB.swap  none  swap  sw 0  0\n</code></pre>"},{"location":"tips_ssd-docker.html","title":"Tips - SSD + Docker","text":"<p>Once you have your Jetson set up by flashing the latest Jetson Linux (L4T) BSP on it or by flashing the SD card with the whole JetPack image, before embarking on testing out all the great generative AI application using <code>jetson-containers</code>, you want to make sure you have a huge storage space for all the containers and the models you will download.  </p> <p>We are going to show how you can install SSD on your Jetson, and set it up for Docker.</p>"},{"location":"tips_ssd-docker.html#ssd","title":"SSD","text":""},{"location":"tips_ssd-docker.html#physical-installation","title":"Physical installation","text":"<ol> <li>Unplug power and any peripherals from the Jetson developer kit.</li> <li>Physically install an NVMe SSD card on the carrier board of your Jetson developer kit, making sure to properly seat the connector and secure with the screw.</li> <li>Reconnect any peripherals, and then reconnect the power supply to turn on the Jetson developer kit.</li> <li> <p>Once the system is up, verify that your Jetson identifies a new memory controller on PCI bus:</p> <pre><code>lspci\n</code></pre> <p>The output should look like the following:</p> <pre><code>0007:01:00.0 Non-Volatile memory controller: Marvell Technology Group Ltd. Device 1322 (rev 02)\n</code></pre> </li> </ol>"},{"location":"tips_ssd-docker.html#format-and-set-up-auto-mount","title":"Format and set up auto-mount","text":"<ol> <li> <p>Run <code>lsblk</code> to find the device name.</p> <pre><code>lsblk\n</code></pre> <p>The output should look like the following:</p> <pre><code>NAME         MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT\nloop0          7:0    0    16M  1 loop \nmmcblk1      179:0    0  59.5G  0 disk \n\u251c\u2500mmcblk1p1  179:1    0    58G  0 part /\n\u251c\u2500mmcblk1p2  179:2    0   128M  0 part \n\u251c\u2500mmcblk1p3  179:3    0   768K  0 part \n\u251c\u2500mmcblk1p4  179:4    0  31.6M  0 part \n\u251c\u2500mmcblk1p5  179:5    0   128M  0 part \n\u251c\u2500mmcblk1p6  179:6    0   768K  0 part \n\u251c\u2500mmcblk1p7  179:7    0  31.6M  0 part \n\u251c\u2500mmcblk1p8  179:8    0    80M  0 part \n\u251c\u2500mmcblk1p9  179:9    0   512K  0 part \n\u251c\u2500mmcblk1p10 179:10   0    64M  0 part \n\u251c\u2500mmcblk1p11 179:11   0    80M  0 part \n\u251c\u2500mmcblk1p12 179:12   0   512K  0 part \n\u251c\u2500mmcblk1p13 179:13   0    64M  0 part \n\u2514\u2500mmcblk1p14 179:14   0 879.5M  0 part \nzram0        251:0    0   1.8G  0 disk [SWAP]\nzram1        251:1    0   1.8G  0 disk [SWAP]\nzram2        251:2    0   1.8G  0 disk [SWAP]\nzram3        251:3    0   1.8G  0 disk [SWAP]\nnvme0n1      259:0    0 238.5G  0 disk \n</code></pre> <p>Identify the device corresponding to your SSD. In this case, it is <code>nvme0n1</code>.</p> </li> <li> <p>Format the SSD, create a mount point, and mount it to the filesystem.</p> <pre><code>sudo mkfs.ext4 /dev/nvme0n1\n</code></pre> <p>You can choose any name for the mount point directory. We use <code>/ssd</code> here, but in <code>jetson-containers</code>' setup.md documentation, <code>/mnt</code> is used.  </p> <pre><code>sudo mkdir /ssd\n</code></pre> <pre><code>sudo mount /dev/nvme0n1 /ssd\n</code></pre> </li> <li> <p>In order to ensure that the mount persists after boot, add an entry to the <code>fstab</code> file:</p> <p>First, identify the UUID for your SSD:</p> <pre><code>lsblk -f\n</code></pre> <p>Then, add a new entry to the <code>fstab</code> file:</p> <pre><code>sudo vi /etc/fstab\n</code></pre> <p>Insert the following line, replacing the UUID with the value found from <code>lsblk -f</code>:</p> <pre><code>UUID=************-****-****-****-******** /ssd/ ext4 defaults 0 2\n</code></pre> </li> <li> <p>Finally, change the ownership of the <code>/ssd</code> directory.</p> <pre><code>sudo chown ${USER}:${USER} /ssd\n</code></pre> </li> </ol>"},{"location":"tips_ssd-docker.html#docker","title":"Docker","text":"<ol> <li> <p>Install the full NVIDIA JetPack SDK, which includes the <code>nvidia-container</code> package.</p> <p>Note: If you used an NVIDIA-supplied SD card image to flash your SD card, all necessary JetPack components are already pre-installed, so this step can be skipped.</p> <pre><code>sudo apt update\nsudo apt install -y nvidia-jetpack\n</code></pre> </li> <li> <p>Restart the Docker service and add your user to the <code>docker</code> group, so that you don't need to use the command with <code>sudo</code>.</p> <pre><code>sudo systemctl restart docker\nsudo usermod -aG docker $USER\nnewgrp docker\n</code></pre> </li> <li> <p>Add default runtime in <code>/etc/docker/daemon.json</code></p> <pre><code>sudo vi /etc/docker/daemon.json\n</code></pre> <p>Insert the <code>\"default-runtime\": \"nvidia\"</code> line as following:</p> <pre><code>{\n\"runtimes\": {\n\"nvidia\": {\n\"path\": \"nvidia-container-runtime\",\n\"runtimeArgs\": []\n}\n},\n\"default-runtime\": \"nvidia\"\n}\n</code></pre> </li> <li> <p>Restart Docker</p> <pre><code>sudo systemctl daemon-reload &amp;&amp; sudo systemctl restart docker\n</code></pre> </li> </ol>"},{"location":"tips_ssd-docker.html#migrate-docker-directory-to-ssd","title":"Migrate Docker directory to SSD","text":"<p>Now that the SSD is installed and available on your device, you can use the extra storage capacity to hold the storage-demanding Docker directory.</p> <ol> <li> <p>Stop the Docker service.</p> <pre><code>sudo systemctl stop docker\n</code></pre> </li> <li> <p>Move the existing Docker folder</p> <pre><code>sudo du -csh /var/lib/docker/ &amp;&amp; \\\nsudo mkdir /ssd/docker &amp;&amp; \\\nsudo rsync -axPS /var/lib/docker/ /ssd/docker/ &amp;&amp; \\\nsudo du -csh  /ssd/docker/ </code></pre> </li> <li> <p>Edit <code>/etc/docker/daemon.json</code></p> <pre><code>sudo vi /etc/docker/daemon.json\n</code></pre> <p>Insert <code>\"data-root\"</code> line like the following.</p> <pre><code>{\n\"runtimes\": {\n\"nvidia\": {\n\"path\": \"nvidia-container-runtime\",\n\"runtimeArgs\": []\n}\n},\n\"default-runtime\": \"nvidia\",\n\"data-root\": \"/ssd/docker\"\n}\n</code></pre> </li> <li> <p>Rename the old Docker data directory</p> <pre><code>sudo mv /var/lib/docker /var/lib/docker.old\n</code></pre> </li> <li> <p>Restart the docker daemon</p> <pre><code>sudo systemctl daemon-reload &amp;&amp; \\\nsudo systemctl restart docker &amp;&amp; \\\nsudo journalctl -u docker\n</code></pre> </li> </ol>"},{"location":"tips_ssd-docker.html#test-docker-on-ssd","title":"Test Docker on SSD","text":"<ol> <li> <p>[Terminal 1] First, open a terminal to monitor the disk usage while pulling a Docker image.</p> <pre><code>watch -n1 df </code></pre> </li> <li> <p>[Terminal 2] Next, open a new terminal and start Docker pull.</p> <pre><code>docker pull nvcr.io/nvidia/l4t-base:r35.2.1\n</code></pre> </li> <li> <p>[Terminal 1] Observe that the disk usage on <code>/ssd</code> goes up as the container image is downloaded and extracted.</p> <pre><code>~$ docker image ls\nREPOSITORY                  TAG       IMAGE ID       CREATED        SIZE\nnvcr.io/nvidia/l4t-base     r35.2.1   dc07eb476a1d   7 months ago   713MB\n</code></pre> </li> </ol>"},{"location":"tips_ssd-docker.html#final-verification","title":"Final Verification","text":"<p>Reboot your Jetson, and verify that you observe the following:</p> <pre><code>~$ sudo blkid | grep nvme\n/dev/nvme0n1: UUID=\"9fc06de1-7cf3-43e2-928a-53a9c03fc5d8\" TYPE=\"ext4\"\n\n~$ df -h\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/mmcblk1p1  116G   18G   94G  16% /\nnone            3.5G     0  3.5G   0% /dev\ntmpfs           3.6G  108K  3.6G   1% /dev/shm\ntmpfs           734M   35M  699M   5% /run\ntmpfs           5.0M  4.0K  5.0M   1% /run/lock\ntmpfs           3.6G     0  3.6G   0% /sys/fs/cgroup\ntmpfs           734M   88K  734M   1% /run/user/1000\n/dev/nvme0n1    458G  824M  434G   1% /ssd\n\n~$ docker info | grep Root\n Docker Root Dir: /ssd/docker\n\n~$ sudo ls -l /ssd/docker/\ntotal 44\ndrwx--x--x  4 root root 4096 Mar 22 11:44 buildkit\ndrwx--x---  2 root root 4096 Mar 22 11:44 containers\ndrwx------  3 root root 4096 Mar 22 11:44 image\ndrwxr-x---  3 root root 4096 Mar 22 11:44 network\ndrwx--x--- 13 root root 4096 Mar 22 16:20 overlay2\ndrwx------  4 root root 4096 Mar 22 11:44 plugins\ndrwx------  2 root root 4096 Mar 22 16:19 runtimes\ndrwx------  2 root root 4096 Mar 22 11:44 swarm\ndrwx------  2 root root 4096 Mar 22 16:20 tmp\ndrwx------  2 root root 4096 Mar 22 11:44 trust\ndrwx-----x  2 root root 4096 Mar 22 16:19 volumes\n\n~$ sudo du -chs /ssd/docker/\n752M    /ssd/docker/\n752M    total\n\n~$ docker info | grep -e \"Runtime\" -e \"Root\"\nRuntimes: io.containerd.runtime.v1.linux nvidia runc io.containerd.runc.v2\n Default Runtime: nvidia\n Docker Root Dir: /ssd/docker\n</code></pre> <p>Your Jetson is now set up with the SSD!</p>"},{"location":"tutorial-intro.html","title":"Tutorial - Intro","text":""},{"location":"tutorial-intro.html#about-jetson","title":"About Jetson","text":"<p>Note</p> <p>We are mainly targeting Orin generation Jetson.</p> Jetson AGX Orin Developer Kit Jetson Orin Nano Developer Kit GPU 2048-core NVIDIA Ampere architecture GPU with 64 Tensor Cores 1024-core NVIDIA Ampere architecture GPU with 32 Tensor Cores RAM(CPU+GPU) 32GB (/ 64GB) 8GB Storage 64GB eMMC (+ NVMe SSD) microSD card (+ NVMe SSD)"},{"location":"tutorial_distillation.html","title":"CLIP model distillation","text":"<p>See \"Jetson Introduction to Knowledge Distillation\" repo's README.md.</p> <p>https://github.com/NVIDIA-AI-IOT/jetson-intro-to-distillation</p>"},{"location":"tutorial_llava.html","title":"Tutorial - LLaVA","text":"<p>Give your locally running LLM an access to vision, by running LLaVA on Jetson!</p> <p></p>"},{"location":"tutorial_llava.html#clone-and-set-up-jetson-containers","title":"Clone and set up <code>jetson-containers</code>","text":"<pre><code>git clone https://github.com/dusty-nv/jetson-containers\ncd jetson-containers\nsudo apt update; sudo apt install -y python3-pip\npip3 install -r requirements.txt\n</code></pre>"},{"location":"tutorial_llava.html#1-use-text-generation-webui-container-to-test-llava-model","title":"1. Use <code>text-generation-webui</code> container to test Llava model","text":"<p>What you need</p> <ol> <li> <p>One of the following Jetson:</p> <p>Jetson AGX Orin 64GB Jetson AGX Orin (32GB)</p> </li> <li> <p>Running one of the following JetPack.5x</p> <p>JetPack 5.1.2 (L4T r35.4.1) JetPack 5.1.1 (L4T r35.3.1) JetPack 5.1 (L4T r35.2.1)</p> </li> <li> <p>Sufficient storage space (preferably with NVMe SSD).</p> <ul> <li><code>6.2GB</code> for <code>text-generation-webui</code> container image</li> <li>Space for models<ul> <li>CLIP model : <code>1.7GB</code></li> <li>Llava-Llama2 merged model : <code>7.3GB</code></li> </ul> </li> </ul> </li> </ol>"},{"location":"tutorial_llava.html#use-text-generation-webui-container-for-web-ui","title":"Use <code>text-generation-webui</code> container for web UI","text":"<p>Using the <code>multimodal</code> extension, you can use the LLaVA model in oobaboonga's <code>text-generation-webui</code>.</p> <pre><code>./run.sh $(./autotag text-generation-webui) /bin/bash -c \\\n  \"python3 /opt/text-generation-webui/download-model.py \\\n  --output=/data/models/text-generation-webui \\\n  liuhaotian/llava-llama-2-13b-chat-lightning-gptq\"\n./run.sh $(./autotag text-generation-webui) /bin/bash -c \\\n  \"cd /opt/text-generation-webui &amp;&amp; python3 server.py --listen \\\n    --model-dir=/data/models/text-generation-webui \\\n    --model=liuhaotian_llava-llama-2-13b-chat-lightning-gptq \\\n    --multimodal-pipeline=llava-llama-2-13b \\\n    --extensions=multimodal \\\n    --chat \\\n    --verbose\"\n</code></pre> <p>Go to Chat tab, drag and drop an image of your choice into the Drop Image Here area, and your question in the text area above and hit Generate button.</p> <p></p>"},{"location":"tutorial_llava.html#result","title":"Result","text":""},{"location":"tutorial_llava.html#2-use-llava-container-to-run-llavaservecli","title":"2. Use <code>llava</code> container to run <code>llava.serve.cli</code>","text":"<p>What you need</p> <ol> <li> <p>One of the following Jetson:</p> <p>Jetson AGX Orin 64GB Jetson AGX Orin (32GB)</p> </li> <li> <p>Running one of the following JetPack.5x</p> <p>JetPack 5.1.2 (L4T r35.4.1) JetPack 5.1.1 (L4T r35.3.1) JetPack 5.1 (L4T r35.2.1)</p> </li> <li> <p>Sufficient storage space (preferably with NVMe SSD).</p> <ul> <li><code>6.1GB</code> for <code>llava</code> container image</li> <li>Space for models<ul> <li>7B model : <code>14GB</code>, or</li> <li>13B model : <code>26GB</code></li> </ul> </li> </ul> </li> </ol> <p>See <code>jetson-containers</code>' <code>llava</code> package README for more infomation**</p>"},{"location":"tutorial_llava.html#llava-llama-2-7b-chat","title":"llava-llama-2-7b-chat","text":"<pre><code>./run.sh --env HUGGING_FACE_HUB_TOKEN=&lt;YOUR-ACCESS-TOKEN&gt; $(./autotag llava) \\\n  python3 -m llava.serve.cli \\\n    --model-path liuhaotian/llava-llama-2-7b-chat-lightning-lora-preview \\\n    --model-base meta-llama/Llama-2-7b-chat-hf \\\n    --image-file /data/images/hoover.jpg\n</code></pre>"},{"location":"tutorial_llava.html#llava-llama-2-13b-chat","title":"llava-llama-2-13b-chat","text":"<p>This may only run on Jetson AGX Orin 64GB.</p> <pre><code>./run.sh $(./autotag llava) \\\n  python3 -m llava.serve.cli \\\n    --model-path liuhaotian/llava-llama-2-13b-chat-lightning-preview \\\n    --image-file /data/images/hoover.jpg\n</code></pre>"},{"location":"tutorial_minigpt4.html","title":"Tutorial - MiniGPT-4","text":"<p>Give your locally running LLM an access to vision, by running MiniGPT-4 on Jetson!</p> <p></p> <p>What you need</p> <ol> <li> <p>One of the following Jetson:</p> <p>Jetson AGX Orin 64GB Jetson AGX Orin (32GB) Jetson Orin Nano Orin (8GB)</p> </li> <li> <p>Running one of the following JetPack.5x</p> <p>JetPack 5.1.2 (L4T r35.4.1) JetPack 5.1.1 (L4T r35.3.1) JetPack 5.1 (L4T r35.2.1)</p> </li> <li> <p>Sufficient storage space (preferably with NVMe SSD).</p> <ul> <li><code>5.8GB</code> for container image</li> <li>Space for pre-quantized MiniGPT-4 model</li> </ul> </li> </ol>"},{"location":"tutorial_minigpt4.html#set-up-a-container-for-minigpt-4","title":"Set up a container for <code>MiniGPT-4</code>","text":"<p>See <code>jetson-containers</code>' <code>minigpt4</code> package README for more infomation**</p>"},{"location":"tutorial_minigpt4.html#clone-and-set-up-jetson-containers","title":"Clone and set up <code>jetson-containers</code>","text":"<pre><code>git clone https://github.com/dusty-nv/jetson-containers\ncd jetson-containers\nsudo apt update; sudo apt install -y python3-pip\npip3 install -r requirements.txt\n</code></pre>"},{"location":"tutorial_minigpt4.html#start-minigpt4-container-with-models","title":"Start <code>minigpt4</code> container with models","text":"<p>To start the MiniGPT4 container and webserver with the recommended models, run this command:</p> <pre><code>cd jetson-containers\n./run.sh $(./autotag minigpt4) /bin/bash -c 'cd /opt/minigpt4.cpp/minigpt4 &amp;&amp; python3 webui.py \\\n  $(huggingface-downloader --type=dataset maknee/minigpt4-13b-ggml/minigpt4-13B-f16.bin) \\\n  $(huggingface-downloader --type=dataset maknee/ggml-vicuna-v0-quantized/ggml-vicuna-13B-v0-q5_k.bin)'\n</code></pre> <p>Then, open your web browser and access <code>http://&lt;IP_ADDRESS&gt;:7860</code>.</p>"},{"location":"tutorial_minigpt4.html#results","title":"Results","text":""},{"location":"tutorial_nanosam.html","title":"Tutorial - NanoSAM","text":"<p>See \"NanoSAM\" repo.</p> <p>https://github.com/NVIDIA-AI-IOT/nanosam</p> <p></p>"},{"location":"tutorial_stable-diffusion.html","title":"Tutorial - Stable Diffusion","text":"<p>Let's run AUTOMATIC1111's <code>stable-diffusion-webui</code> on NVIDIA Jetson.</p> <p></p> <p>What you need</p> <ol> <li> <p>One of the following Jetson:</p> <p>Jetson AGX Orin 64GB Jetson AGX Orin (32GB) Jetson Orin Nano Orin (8GB)[^1]</p> </li> <li> <p>Running one of the following JetPack.5x</p> <p>JetPack 5.1.2 (L4T r35.4.1) JetPack 5.1.1 (L4T r35.3.1) JetPack 5.1 (L4T r35.2.1)</p> </li> <li> <p>Sufficient storage space (preferably with NVMe SSD).</p> <ul> <li><code>6.8GB</code> for container image</li> <li>Spaces for models</li> </ul> </li> </ol>"},{"location":"tutorial_stable-diffusion.html#set-up-a-container-for-stable-diffusion-webui","title":"Set up a container for <code>stable-diffusion-webui</code>","text":""},{"location":"tutorial_stable-diffusion.html#clone-jetson-containers","title":"Clone <code>jetson-containers</code>","text":"<p>See <code>jetson-containers</code>' <code>stable-diffusion-webui</code> package README for more infomation**</p> <pre><code>git clone https://github.com/dusty-nv/jetson-containers\ncd jetson-containers\nsudo apt update; sudo apt install -y python3-pip\npip3 install -r requirements.txt\n</code></pre> <p>Info</p> <p>JetsonHacks provides an informative walkthrough video on <code>jetson-containers</code>, showcasing the usage of both the <code>stable-diffusion-webui</code> and <code>text-generation-webui</code> containers.</p> <p>You can find the complete article with detailed instructions here.</p> <p></p>"},{"location":"tutorial_stable-diffusion.html#how-to-start","title":"How to start","text":"<p>Use <code>run.sh</code> and <code>autotag</code> script to automatically pull or build a compatible container image.</p> <pre><code>cd jetson-containers\n./run.sh $(./autotag stable-diffusion-webui)\n</code></pre> <p>For other ways to start the container, check the README of <code>jetson-containers</code>' <code>stable-diffusion-webui</code> package.</p> <p>The container has a default run command (<code>CMD</code>) that will automatically start the webserver like this:</p> <pre><code>cd /opt/stable-diffusion-webui &amp;&amp; python3 launch.py \\\n  --data=/data/models/stable-diffusion \\\n  --enable-insecure-extension-access \\\n  --xformers \\\n  --listen \\\n  --port=7860\n</code></pre> <p>You should see it's downloading the model checkpoint on the first run.</p> <p>Open your browser and access <code>http://&lt;IP_ADDRESS&gt;:7860</code>.</p>"},{"location":"tutorial_stable-diffusion.html#results-output-examples","title":"Results / Output Examples","text":"<p>Want to explore using Python APIs to run diffusion models directly? See <code>jetson-containers</code>.</p>"},{"location":"tutorial_text-generation.html","title":"Tutorial - text-generation-webui","text":"<p>Interact with a local AI assistant by running a LLM with oobabooga's <code>text-generaton-webui</code> on NVIDIA Jetson!</p> <p></p> <p>What you need</p> <ol> <li> <p>One of the following Jetson:</p> <p>Jetson AGX Orin 64GB Jetson AGX Orin (32GB) Jetson Orin Nano Orin (8GB)1</p> </li> <li> <p>Running one of the following JetPack.5x</p> <p>JetPack 5.1.2 (L4T r35.4.1) JetPack 5.1.1 (L4T r35.3.1) JetPack 5.1 (L4T r35.2.1)</p> </li> <li> <p>Sufficient storage space (preferably with NVMe SSD).</p> <ul> <li><code>6.2GB</code> for container image</li> <li>Spaces for models</li> </ul> </li> </ol>"},{"location":"tutorial_text-generation.html#set-up-a-container-for-text-generation-webui","title":"Set up a container for <code>text-generation-webui</code>","text":""},{"location":"tutorial_text-generation.html#clone-jetson-containers","title":"Clone <code>jetson-containers</code>","text":"<p>See <code>jetson-containers</code>' <code>text-generation-webui</code> package README for more infomation**</p> <pre><code>git clone https://github.com/dusty-nv/jetson-containers\ncd jetson-containers\nsudo apt update; sudo apt install -y python3-pip\npip3 install -r requirements.txt\n</code></pre> <p>Info</p> <p>JetsonHacks provides an informative walkthrough video on <code>jetson-containers</code>, showcasing the usage of both the <code>stable-diffusion-webui</code> and <code>text-generation-webui</code> containers.</p> <p>You can find the complete article with detailed instructions here.</p> <p></p>"},{"location":"tutorial_text-generation.html#how-to-start","title":"How to start","text":"<p>If you are running this for the first time, go through the pre-setup.</p> <p>Use <code>run.sh</code> and <code>autotag</code> script to automatically pull or build a compatible container image.</p> <pre><code>cd jetson-containers\n./run.sh $(./autotag text-generation-webui)\n</code></pre> <p>For other ways to start the container, check the README of <code>jetson-containers</code>' <code>text-generation-webui</code> package.</p> <p>The container has a default run command (<code>CMD</code>) that will automatically start the webserver like this:</p> <pre><code>cd /opt/text-generation-webui &amp;&amp; python3 server.py \\\n  --model-dir=/data/models/text-generation-webui \\\n  --chat \\\n  --listen\n</code></pre> <p>Open your browser and access <code>http://&lt;IP_ADDRESS&gt;:7860</code>.</p>"},{"location":"tutorial_text-generation.html#download-a-model-on-web-ui","title":"Download a model on web UI","text":"<p>On the web UI, select Model tab and navigate to \"Download model or LoRA\" section.</p> <p>Enter the Hugging Face username/model path (that you can click on Hugging Face model repo page to click to copy to your clipboard).</p>"},{"location":"tutorial_text-generation.html#ggml-models","title":"GGML models","text":"<p>The fastest model loader to use is currently llama.cpp with 4-bit quantized GGML models.</p> <p>You can download a single model file for a particular quantization, like <code>*.a4_0.bin</code>. Input the file name and hit \"Download\" button.</p> <p></p> <p>Info</p>"},{"location":"tutorial_text-generation.html#model-selection-for-jetson-orin-nano","title":"Model selection for Jetson Orin Nano","text":"<p>Jetson Orin Nano Developer Kit has only 8GB RAM for both CPU (system) and GPU, so you need to pick a model that fits in the RAM size.</p> <p>7 billion parameter models typically takes up about 4GB if it uses 4-bit quantization, and that's probably the biggest you can run on Jetson Orin Nano.</p> <p>Make sure you go through the RAM optimization steps before attempting to load such model on Jetson Orin Nano.</p> <p>It would still take more than 10 minutes to load the model as it needs to first load everything to CPU memory and then shuffle it down to GPU memory using swap.</p>"},{"location":"tutorial_text-generation.html#load-a-model","title":"Load a model","text":"<p>After clicking \ud83d\udd04 button to refresh your model list, select the model you want to use.</p> <p>For a GGML model, remember to</p> <ul> <li>Set <code>n-gpu-layers</code> to <code>128</code></li> <li>Set <code>n_gqa</code> to <code>8</code> if you using Llama-2-70B (on Jetson AGX Orin 64GB)</li> </ul>"},{"location":"tutorial_text-generation.html#results","title":"Results","text":""},{"location":"tutorial_text-generation.html#model-size-tested","title":"Model size tested","text":"<p>With llama.cpp, GGML model, 4-bit quantization.</p> Model size Jetson AGX Orin 64GB Jetson AGX Orin 32GB Jetson Orin Nano 8GB 70B model \u2705 30B model \u2705 \u2705 13B model \u2705 \u2705 7B model \u2705 \u2705 \u2705 <p>Want to explore using Python APIs to run LLMs directly?  See <code>jetson-containers</code> for its LLM related packages and containers.</p> <ol> <li> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit.\u00a0\u21a9</p> </li> </ol>"}]}